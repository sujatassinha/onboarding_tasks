# External Imports
import numpy as np
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch.nn as nn
import gc

# Internal Imports
from llm_attacks import get_embedding_matrix, get_embeddings

def token_gradients(model, input_ids, input_slice, target_slice, loss_slice):

  """
  Compute gradients of the loss with respect to the coordinates.

  Parameters:
  -----------
  model : torch.nn.Module
      The pre-trained language model.
  input_ids : torch.Tensor
      Tensor containing the token IDs of the input text.
  input_slice : slice
      Slice object of the input sequence specifying the range of tokens to compute gradients for.
  target_slice : slice
      Slice object of the input sequence specifying the range of tokens used as target.
  loss_slice : slice
      Slice object of the logits specifying the range of logits to compute the loss over.

  Returns:
  -------
  torch.Tensor
      Gradients of each token in the input_slice with respect to the loss.
  """

  # Get the embedding matrix of the model
  # Note: Embedding matrix are trainable parameters
  # ... these are the weights learned by a model during training 
  # ... in the same way a model learns weights for a dense layer
  embed_weights = get_embedding_matrix(model)

  # Create a one-hot encoded tensor for the input tokens in the input slice range
  # Note: this tensor will be used to create one-hot representation of input tokens
  # ... within the specified input_slice
  one_hot = torch.zeros(
    input_ids[input_slice].shape[0], # No. of tokens in the input slice
    embed_weights.shape[0],          # Size of the vocabulary
    device=model.device,             # Device on which to create the tensor
    dtype=embed_weights.dtype        # Datatype of the tensor        
  )

  # Fill the one-hot tensor with 1s at position corresponding to input_ids
  one_hot.scatter_(
    1, 
    input_ids[input_slice].unsqueeze(1),
    torch.ones(one_hot.shape[0], 1, device=model.device, dtype=embed_weights.dtype)
  )
  # Enable gradient computation for one-hot tensor
  one_hot.requires_grad_()

  # Compute input_embeddings by multiplying one-hot tensor with embedding weights
  # Note: Here we obtain all values where one_hot is 1 (ie at all input_ids)
  input_embeds = (one_hot @ embed_weights).unsqueeze(0)

  # Get the full embedding for the input_ids
  # Note: Using detach() to ensure that the embeddings are not a part of the computational graphs
  embeds = get_embeddings(model, input_ids.unsqueeze(0)).detach()
  
  # Replace the input embeddings in the input slice range with the newly computed input embeddings
  full_embeds = torch.cat(
    [
      embeds[:,:input_slice.start,:], # Embeddings before input slice
      input_embeds,                   # New input embeddings
      embeds[:, input_slice.stop:,:]  # Embeddings after input slice
    ],
    dim=1                             # Comcatenate along the token dimension
  )
  # Compute logits using the model with the full embeddings
  logits = model(inputs_embeds=full_embeds).logits
  # Extract the target token IDs for loss calculation
  targets = input_ids[target_slice]
  # Compute the cross-entropy loss between logits and targets
  loss = nn.CrossEntropyLoss()(logits[0,loss_slice,:], targets)

  # Backpropagate the loss to compute gradients
  loss.backward()

  # Clone the gradients from one-hot tensor
  grad = one_hot.grad.clone()
  # Normalize the gradients
  grad = grad/grad.norm(dim=-1, keepdim=True)
  # Return the normalized gradients
  return grad


def sample_control(control_toks, grad, batch_size, topk=256, temp=1, not_allowed_tokens=None):

  """
  Generate new control tokens by sampling based on gradients.

  Parameters:
  -----------
  control_toks : torch.Tensor
      The initial control tokens.
  grad : torch.Tensor
      The gradients to be used for sampling.
  batch_size : int
      The number of samples to generate.
  topk : int, optional
      The number of top gradient values to consider for sampling (default is 256).
  temp : float, optional
      A temperature parameter for sampling (default is 1, not used directly in this code).
  not_allowed_tokens : torch.Tensor, optional
      Tokens that should not be sampled (default is None).

  Returns:
  -------
  torch.Tensor
      The new control tokens generated by sampling.
  """

  # For all the not allowed tokens, set the gradients to infinity to ensure that they are not selected 
  if not_allowed_tokens is not None:
    grad[:, not_allowed_tokens.to(grad.device)] = np.inf
  
  # Negate the gradients and select the 'topk' indices for each gradient row
  top_indices = (-grad).topk(topk, dim=1).indices

  # Move control tokens to the same device as gradients
  control_toks = control_toks.to(grad.device)

  # Repeat control tokens for specified batch size 
  original_control_toks = control_toks.repeat(batch_size, 1)
  
  # Create a tensor new_token_pos to specify the positions where new tokens will be inserted. 
  # ... This divides the length of control_toks evenly across the batch_size.
  new_token_pos = torch.arange(0,
  len(control_toks),
  len(control_toks) / batch_size,
  device=grad.device
  ).type(torch.int64)

  # For each position in new_token_pos, sample a new token from the top-k tokens. 
  # ... torch.gather is used to select these tokens based on randomly chosen indices.
  new_token_val = torch.gather(
    top_indices[new_token_pos], 1,
    torch.randint(0, topk, (batch_size, 1), device=grad.device)
  )

  # Insert the new token values into the original control tokens at the positions specified by new_token_pos.
  # ... torch.scatter_ is used to update the control tokens.
  new_control_toks = original_control_toks.scatter_(1, 
  new_token_pos.unsqueeze(-1),
  new_token_val
  )

  # Return new control tokens
  return new_control_toks

def get_filtered_cands(tokenizer, control_candidates, filter_candidates=True, curr_control=None):

  """
  Filter and decode candidate control tokens.

  Parameters:
  -----------
  tokenizer : transformers.PreTrainedTokenizer
      The tokenizer used to decode the token IDs.
  control_candidates : torch.Tensor
      Tensor containing candidate control token IDs.
  filter_candidates : bool, optional
      Whether to filter candidates based on certain criteria (default is True).
  curr_control : str, optional
      The current control token string to avoid duplicates (default is None).

  Returns:
  -------
  list of str
      A list of decoded and possibly filtered candidate control tokens.
  """

  # Initialize an empty list to store candidates,
  # ... and a count for filtered out candidates
  candidates, count = [], 0

  # Iterate over each candidate in the control candidates 
  for i in range(control_candidates.shape[0]):

    # Decode the candidate token IDs to a string
    decoded_str = tokenizer.decode(control_candidates[i], skip_special_tokens=True)
    
    if filter_candidates:
      # If filtering is enabled, check if the decoded string is valid
      if decoded_str != curr_control and len(tokenizer(decoded_str, add_special_tokens=False).input_ids) == len(control_candidates[i]):
        candidates.append(decoded_str) # Add valid decoded string to candidates list
      else:
        count += 1                # Increment count for invalid candidates
    
    else:
      # If filtering is disabled, add the decoded string directly to candidates list
      candidates.append(decoded_str)

  if filter_candidates:
    # If filtering is enabled, pad the candidates list to match the length of control_candidates
    candidates = candidates + [candidates[-1]] * (len(control_candidates) - len(candidates))

  # Return the final list of candidates
  return candidates

def get_logits(*, model, tokenizer, input_ids, control_slice, test_controls=None, return_ids=False, batch_size=512):
  
  """
  Generate logits for modified input sequences by inserting control tokens.

  Parameters:
  -----------
  model : torch.nn.Module
      The pre-trained language model.
  tokenizer : transformers.PreTrainedTokenizer
      The tokenizer used to encode control tokens.
  input_ids : torch.Tensor
      Tensor containing the token IDs of the input text.
  control_slice : slice
      Slice object specifying the range of tokens to be replaced by control tokens.
  test_controls : list of str, optional
      List of control token strings to be tested (default is None).
  return_ids : bool, optional
      Whether to return the modified input IDs along with logits (default is False).
  batch_size : int, optional
      The number of samples to process in each batch (default is 512).

  Returns:
  -------
  torch.Tensor
      The logits generated by the model.
  torch.Tensor, optional
      The modified input IDs if `return_ids` is True.

  Raises:
  ------
  ValueError
      If `test_controls` is not a list of strings or if the shape of control tokens is incorrect.
  """

  # Check if test_controls is a list of strings
  if isinstance(test_controls[0], str):
    # Determine the maximum length of the control tokens to be inserted
    max_len = control_slice.stop - control_slice.start

    # Encode the control tokens using the tokenizer and truncate to max_length
    test_ids = [
      torch.tensor(tokenizer(control, add_special_tokens=False).input_ids[:max_len], device=model.device)
      for control in test_controls
    ]

    # Find a padding token ID that is not in input_ids or test_ids
    pad_tok=0
    while pad_tok in input_ids or any([pad_tok in ids for ids in test_ids]):
      pad_tok+=1
    
    # Convert the list of control token IDs into padded tensor
    nested_ids = torch.nested.nested_tensor(test_ids)
    test_ids = torch.nested.to_padded_tensor(nested_ids, pad_tok, (len(test_ids), max_len))

  else:
    raise ValueError(f"test_controls must be a list of strings, got {type(test_controls)}")
  
  # Ensure the control tokens have the correct shape
  if not(test_ids[0].shape[0] == control_slice.stop - control_slice.start):
    raise ValueError((
      f"test_controls must have shape "
      f"(n, {control_slice.stop - control_slice.start}), "
      f"got {test_ids.shape}"
    ))

  # Generate locations for inserting control tokens into input_ids
  locs = torch.arange(control_slice.start, control_slice.stop).repeat(test_ids.shape[0], 1).to(model.device)
    
  # Create modified input_ids by scattering control tokens into the input sequence
  ids = torch.scatter(
    input_ids.unsqueeze(0).repeat(test_ids.shape[0], 1).to(model.device),
    1,
    locs,
    test_ids
  )

  # Create attention mask if padding token is non negative
  if pad_tok >= 0:
    attn_mask = (ids != pad_tok).type(ids.dtype)
  else:
    attn_mask = None

  # Forward pass through the model to get logits
  if return_ids:
    # If return_ids is True, return logits and modified input IDs.
    del locs, test_ids; gc.collect()
    logits = forward(model=model, input_ids=ids, attention_mask=attn_mask, batch_size=batch_size)
    return logits, ids
  else:
    # Otherwise, return only logits.
    del locs, test_ids
    logits = forward(model=model, input_ids=ids, attention_mask=attn_mask, batch_size=batch_size)
    del ids; gc.collect()
    return logits

def forward(*, model, input_ids, attention_mask, batch_size=512):

  """
  Forward pass to compute logits for input sequences in batches.

  Parameters:
  -----------
  model : torch.nn.Module
      The pre-trained language model.
  input_ids : torch.Tensor
      Tensor containing the token IDs of the input text.
  attention_mask : torch.Tensor, optional
      Attention mask to avoid attending to padding tokens (default is None).
  batch_size : int, optional
      The number of samples to process in each batch (default is 512).

  Returns:
  -------
  torch.Tensor
      The logits generated by the model.
  """

  # Initalize a list of store logits for each batch
  logits = []

  # Iterate over input_ids in batches
  for i in range(0, input_ids.shape[0], batch_size):
    # Extract a batch of input_ids
    batch_input_ids = input_ids[i:i+batch_size]
    
    # Extract the corresponding attention mask if provided 
    if attention_mask is not None:
      batch_attention_mask = attention_mask[i:i+batch_size]
    else:
      batch_attention_mask = None

    # Compute logits for the current batch and append it to the list
    logits.append(model(input_ids=batch_input_ids, attention_mask=batch_attention_mask).logits)

    # Free up memory
    gc.collect()
    del batch_input_ids, batch_attention_mask
  
  # Concatenate logits from all batches and return
  return torch.cat(logits, dim=0)

def target_loss(logits, ids, target_slice):

  """
  Compute the cross-entropy loss for the target tokens.

  Parameters:
  -----------
  logits : torch.Tensor
      The logits output from the model, of shape (batch_size, sequence_length, vocab_size).
  ids : torch.Tensor
      The tensor containing the token IDs of the input text, of shape (batch_size, sequence_length).
  target_slice : slice
      Slice object specifying the range of tokens to compute the loss for.

  Returns:
  -------
  torch.Tensor
      The mean cross-entropy loss for each sequence in the batch.
  """

  # Define the criterion for cross-entropy loss without reduction
  criterion = nn.CrossEntropyLoss(reduction='none')

  # Adjust the target slice to match the positions in logits
  loss_slice = slice(target_slice.start-1, target_slice.stop-1)
  # Compute the cross-entropy loss for the specified slice
  # Transpose logits to match the expected input shape for CrossEntropyLoss
  loss = criterion(logits[:, loss_slice, :].transpose(1,2), ids[:, target_slice])

  # Return the mean loss for each sequence in the batch
  return loss.mean(dim=-1)


# although this function is defined, it is not needed
def load_model_and_tokenizer(model_path, access_token, tokenizer_path=None, device='cuda:0', **kwargs):
  
  """
  Load a pre-trained model and its corresponding tokenizer.

  Parameters:
  -----------
  model_path : str
      Path to the pre-trained model directory.
  tokenizer_path : str, optional
      Path to the pre-trained tokenizer directory. If None, defaults to the model path.
  device : str, optional
      The device to load the model onto. Default is 'cuda:0'.
  **kwargs : dict, optional
      Additional keyword arguments passed to the model loader.

  Returns:
  --------
  model : AutoModelForCausalLM
      The loaded pre-trained model.
  tokenizer : AutoTokenizer
      The loaded tokenizer associated with the model.

  Notes:
  ------
  - The model is loaded with torch_dtype set to torch.float16.
  - `trust_remote_code` is set to True for both model and tokenizer.
  - If `tokenizer_path` is not provided, it defaults to the `model_path`.
  - If the tokenizer does not have a `pad_token`, it defaults to `eos_token`.
  """

  # Load Model
  model = AutoModelForCausalLM.from_pretrained(
    model_path,
    token=access_token,
    torch_dtype=torch.float16,
    trust_remote_code=True,
    **kwargs
  ).to(device).eval()

  # Specify Tokenizer path
  tokenizer_path = model if tokenizer_path is None else tokenizer_path

  # Load Tokenizer
  tokenizer = AutoTokenizer.from_pretrained(
    tokenizer_path,
    token=access_token,
    trust_remote_code=True,
    use_fast=False
  )

  if 'llama-2' in tokenizer_path:
    tokenizer.pad_token = tokenizer.unk_token
    tokenizer.padding_side = 'left'

  if not tokenizer.pad_token:
    tokenizer.pad_token = tokenizer.eos_token

  return model, tokenizer
  



  
