Average sequence length: 91.34038650085759
Min, Median, Max sequence length: [1, 72, 1151]
Number of indices to drop for input length: 132635
Average sequence length: 99.35036414859839
Min, Median, Max sequence length: [4, 84, 1151]
Number of indices to drop for output length: 118172
Initial Dataset is of shape: (2095985, 4)
Final   Dataset is of shape: (1940163, 4)
Shapes for, Train Dataset: (1552130, 4), Test Dataset: (388033, 4)
/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5_fast.py:171: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.
For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.
- Be aware that you SHOULD NOT rely on google-t5/t5-base automatically truncating your input to 512 when padding/encoding.
- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.
- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.
  warnings.warn(
['input_ids', 'attention_mask', 'labels']
['input_ids', 'attention_mask', 'labels']
trainable params: 4065280 || all params: 156189952 || trainable%: 2.6027794668891375
Initiating Fine-Tuning for the model on our dataset
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
Epoch: 0, Loss:  11.622883796691895
Epoch: 0, Loss:  10.22712230682373
Epoch: 0, Loss:  8.078218460083008
Epoch: 0, Loss:  6.365673542022705
Epoch: 0, Loss:  5.1073408126831055
Epoch: 0, Loss:  3.909359931945801
Epoch: 0, Loss:  2.8939945697784424
Epoch: 0, Loss:  2.1322786808013916
Epoch: 0, Loss:  1.6618729829788208
Epoch: 0, Loss:  1.3775534629821777
Epoch: 0, Loss:  1.245094656944275
Epoch: 0, Loss:  1.1764224767684937
Epoch: 0, Loss:  1.1487962007522583
Epoch: 0, Loss:  1.078238844871521
Epoch: 0, Loss:  1.1120160818099976
Epoch: 0, Loss:  1.035798192024231
Epoch: 0, Loss:  1.0628392696380615
Epoch: 0, Loss:  0.9751667976379395
Epoch: 0, Loss:  0.9580904841423035
Epoch: 0, Loss:  1.0396548509597778
Epoch: 0, Loss:  0.8956809639930725
Epoch: 0, Loss:  0.9771332740783691
Epoch: 0, Loss:  0.8546877503395081
Epoch: 0, Loss:  0.9309817552566528
Epoch: 0, Loss:  0.8955720663070679
Epoch: 0, Loss:  0.9163973927497864
Epoch: 0, Loss:  0.8155872821807861
Epoch: 0, Loss:  0.8487290143966675
Epoch: 0, Loss:  0.8346587419509888
Epoch: 0, Loss:  0.8116699457168579
Epoch: 0, Loss:  0.847495436668396
Epoch: 0, Loss:  0.8012860417366028
Epoch: 0, Loss:  0.7993212342262268
Epoch: 0, Loss:  0.845980167388916
Epoch: 0, Loss:  0.7893161177635193
Epoch: 0, Loss:  0.7196370959281921
Epoch: 0, Loss:  0.855771541595459
Epoch: 0, Loss:  0.7151873707771301
Epoch: 0, Loss:  0.69977205991745
Epoch: 0, Loss:  0.7440788745880127
Epoch: 0, Loss:  0.721807599067688
Epoch: 0, Loss:  0.685352623462677
Epoch: 0, Loss:  0.7296149730682373