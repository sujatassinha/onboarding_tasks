Average sequence length: 91.34038650085759
Min, Median, Max sequence length: [1, 72, 1151]
Number of indices to drop for input length: 132635
Average sequence length: 99.35036414859839
Min, Median, Max sequence length: [4, 84, 1151]
Number of indices to drop for output length: 118172
Initial Dataset is of shape: (2095985, 4)
Final   Dataset is of shape: (1940163, 4)
Shapes for, Train Dataset: (1552130, 4), Test Dataset: (388033, 4)
['input_ids', 'attention_mask', 'labels']
['input_ids', 'attention_mask', 'labels']
trainable params: 4065280 || all params: 156189952 || trainable%: 2.6027794668891375
Initiating Fine-Tuning for the model on our dataset
Epoch: 0
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
Loss:  11.622883796691895
Loss:  1.1370434761047363
Loss:  0.8418352007865906
Loss:  0.7379652857780457
Loss:  0.6570279598236084
Loss:  0.5514339208602905
Loss:  0.5649956464767456
Loss:  0.5749770402908325
Loss:  0.4888269603252411
Loss:  0.5027675032615662
Loss:  0.4849564731121063
Loss:  0.5178415775299072
Loss:  0.4924772381782532
Loss:  0.512355625629425
Time : 1703.4873778820038 secs
Epoch: 1
Loss:  0.5719186663627625
Loss:  0.523298978805542
Loss:  0.46953222155570984
Loss:  0.4754849672317505
Loss:  0.4760311543941498
Loss:  0.4485892951488495
Loss:  0.45562902092933655
Loss:  0.5007117390632629
Loss:  0.38989266753196716
Loss:  0.4390287697315216
Loss:  0.43093058466911316
Loss:  0.4676705598831177
Loss:  0.45406031608581543
Loss:  0.44518232345581055
Time : 1706.736218214035 secs
Epoch: 2
Loss:  0.5174921751022339
Loss:  0.48696160316467285
Loss:  0.44033029675483704
Loss:  0.443538099527359
Loss:  0.4403114318847656
Loss:  0.4190007448196411
Loss:  0.4211224913597107
Loss:  0.4695132374763489
Loss:  0.3773844242095947
Loss:  0.41120702028274536
Loss:  0.41678351163864136
Loss:  0.44238582253456116
Loss:  0.42241939902305603
Loss:  0.42751094698905945
Time : 1705.4655458927155 secs
Epoch: 3
Loss:  0.49704766273498535
Loss:  0.47256672382354736
Loss:  0.4094015657901764
Loss:  0.4262097179889679
Loss:  0.4290701448917389
Loss:  0.4010501205921173
Loss:  0.4033149480819702
Loss:  0.45538291335105896
Loss:  0.36573225259780884
Loss:  0.39935582876205444
Loss:  0.40258824825286865
Loss:  0.4311344623565674
Loss:  0.41811859607696533
Loss:  0.41148361563682556
Time : 1706.6030066013336 secs
Epoch: 4
Loss:  0.48023608326911926
Loss:  0.4500730335712433
Loss:  0.3993150293827057
Loss:  0.40840956568717957
Loss:  0.4114709198474884
Loss:  0.39124760031700134
Loss:  0.3912838101387024
Loss:  0.43116384744644165
Loss:  0.35782769322395325
Loss:  0.3915851414203644
Loss:  0.38680675625801086
Loss:  0.4227529466152191
Loss:  0.4150603115558624
Loss:  0.40744632482528687
Time : 1703.1079995632172 secs
Epoch: 5
Loss:  0.46909821033477783
Loss:  0.4498452842235565
Loss:  0.39869484305381775
Loss:  0.3968406021595001
Loss:  0.40677082538604736
Loss:  0.3747127950191498
Loss:  0.3876233994960785
Loss:  0.4233524203300476
Loss:  0.34423431754112244
Loss:  0.3812287151813507
Loss:  0.3804011046886444
Loss:  0.41015082597732544
Loss:  0.39427584409713745
Loss:  0.401571661233902
Time : 1701.9897260665894 secs
Epoch: 6
Loss:  0.4674724042415619
Loss:  0.43827375769615173
Loss:  0.3739866614341736
Loss:  0.40261879563331604
Loss:  0.4066867530345917
Loss:  0.3719484210014343
Loss:  0.37476488947868347
Loss:  0.4326135516166687
Loss:  0.3350197672843933
Loss:  0.377503901720047
Loss:  0.3650287985801697
Loss:  0.39845019578933716
Loss:  0.3950526714324951
Loss:  0.38216063380241394
Time : 1703.8002045154572 secs
Epoch: 7
Loss:  0.4557657539844513
Loss:  0.4301970303058624
Loss:  0.38394030928611755
Loss:  0.392040878534317
Loss:  0.40153375267982483
Loss:  0.36784330010414124
Loss:  0.37674015760421753
Loss:  0.41369757056236267
Loss:  0.33462509512901306
Loss:  0.36078885197639465
Loss:  0.36027753353118896
Loss:  0.3924787938594818
Loss:  0.3844951391220093
Loss:  0.37507256865501404
Time : 1704.6817548274994 secs
Epoch: 8
Loss:  0.4410943388938904
Loss:  0.42451226711273193
Loss:  0.3679952323436737
Loss:  0.3832191228866577
Loss:  0.3924063742160797
Loss:  0.36282727122306824
Loss:  0.36929622292518616
Loss:  0.42314717173576355
Loss:  0.32537880539894104
Loss:  0.35537657141685486
Loss:  0.36263546347618103
Loss:  0.37532535195350647
Loss:  0.3823462724685669
Loss:  0.3705644905567169
Time : 1703.3776986598969 secs
Epoch: 9
Loss:  0.4503784775733948
Loss:  0.4137464165687561
Loss:  0.3647514283657074
Loss:  0.37564730644226074
Loss:  0.38929587602615356
Loss:  0.35783928632736206
Loss:  0.36327195167541504
Loss:  0.39872634410858154
Loss:  0.30454838275909424
Loss:  0.36258789896965027
Loss:  0.355938583612442
Loss:  0.3870515823364258
Loss:  0.38484179973602295
Loss:  0.37373679876327515
Time : 1704.8123941421509 secs
Epoch: 10
Loss:  0.44071370363235474
Loss:  0.40672409534454346
Loss:  0.3643358647823334
Loss:  0.36742669343948364
Loss:  0.3928202986717224
Loss:  0.3519136905670166
Loss:  0.36146700382232666
Loss:  0.40404972434043884
Loss:  0.32197657227516174
Loss:  0.356254905462265
Loss:  0.3603202998638153
Loss:  0.38220807909965515
Loss:  0.3719066083431244
Loss:  0.3708702325820923
Time : 1704.1807639598846 secs
Epoch: 11
Loss:  0.4349156320095062
Loss:  0.3994191586971283
Loss:  0.3545098900794983
Loss:  0.3683285713195801
Loss:  0.38391053676605225
Loss:  0.35326245427131653
Loss:  0.3554510474205017
Loss:  0.40154972672462463
Loss:  0.3102772533893585
Loss:  0.3591584861278534
Loss:  0.3508482575416565
Loss:  0.3810495436191559
Loss:  0.3696589767932892
Loss:  0.35469850897789
Time : 1704.786553144455 secs
Epoch: 12
Loss:  0.4344785809516907
Loss:  0.40027472376823425
Loss:  0.3492211401462555
Loss:  0.3663727641105652
Loss:  0.36986956000328064
Loss:  0.33864691853523254
Loss:  0.3436993360519409
Loss:  0.38325178623199463
Loss:  0.30932971835136414
Loss:  0.3492495119571686
Loss:  0.3467991054058075
Loss:  0.36538997292518616
Loss:  0.37339192628860474
Loss:  0.36816439032554626
Time : 1702.9814355373383 secs
Epoch: 13
Loss:  0.4370497465133667
Loss:  0.3949206471443176
Loss:  0.3470224440097809
Loss:  0.3598458170890808
Loss:  0.3736691176891327
Loss:  0.3451909124851227
Loss:  0.3521067500114441
Loss:  0.38435545563697815
Loss:  0.31532901525497437
Loss:  0.3429808020591736
Loss:  0.3325261175632477
Loss:  0.3527631163597107
Loss:  0.3636256754398346
Loss:  0.36156266927719116
Time : 1705.3293516635895 secs
Epoch: 14
Loss:  0.417442262172699
Loss:  0.3951781690120697
Loss:  0.35087940096855164
Loss:  0.35939207673072815
Loss:  0.3768262267112732
Loss:  0.34191590547561646
Loss:  0.3389457166194916
Loss:  0.3862345218658447
Loss:  0.3030627965927124
Loss:  0.3476719558238983
Loss:  0.3316706717014313
Loss:  0.3560170829296112
Loss:  0.35829299688339233
Loss:  0.3521943688392639
Time : 1705.6950252056122 secs
Now generating paraphrases on our fine tuned model for the validation dataset and saving it in a dataframe
Completed 0
Completed 64
Output Files generated for review
/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1510: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.
